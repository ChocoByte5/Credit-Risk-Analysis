# -*- coding: utf-8 -*-
"""credit-risk-analysis-DT-Tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cvE-ObdINK6uzce9zhZmeMH_ux3LbA22
"""

# Import bibliotek
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Pobranie danych
import kagglehub

path = kagglehub.dataset_download("taweilo/loan-approval-classification-data")

print("Path to dataset files:", path)

df = pd.read_csv(path + "/loan_data.csv")
print(df.head())  # Podgląd pierwszych 5 wierszy

print(df.columns)

df.drop(columns=["person_gender","person_emp_exp","loan_intent"], inplace=True, errors="ignore")

df["previous_loan_defaults_on_file"] = df["previous_loan_defaults_on_file"].astype(str).str.strip()

# Mapowanie
df["previous_loan_defaults_on_file"] = df["previous_loan_defaults_on_file"].map({"No": 0, "Yes": 1})
# Uzupełnienie brakujących wartości NaN zerami i konwersja na int
df["previous_loan_defaults_on_file"] = df["previous_loan_defaults_on_file"].fillna(0).astype(int)

# Sprawdzenie czy konwersja się udała
print(df["previous_loan_defaults_on_file"].unique())  # Powinno pokazać tylko [0, 1]
print(df["previous_loan_defaults_on_file"].dtype)  # Powinno być int64

print(df["previous_loan_defaults_on_file"].unique())

df["person_home_ownership"] = df["person_home_ownership"].map({
    "RENT": 0,        # Najmniejsza stabilność finansowa
    "MORTGAGE": 1,    # Kredyt hipoteczny, ale zadłużenie
    "OWN": 2,         # Największa stabilność finansowa
    "Other": 1        # Traktujemy podobnie jak mortgage
})

df["person_education"] = df["person_education"].map({
    "High School": 0,      # Najniższe wykształcenie i niższe zarobki
    "Associate": 1,        # Średni poziom wykształcenia
    "Bachelor": 2,         # Wyższe wykształcenie i większa szansa na zarobki
    "Other": 1             # Traktujemy jak "Associate" bo nie wiemy dokładnie
})

cechy_wazne = ["person_income", "loan_amnt", "loan_int_rate",
               "cb_person_cred_hist_length", "credit_score",
               "previous_loan_defaults_on_file", "person_age",
               "person_home_ownership" ,"loan_percent_income",
               "person_education"]

#Cechy i etykiety
X = df[cechy_wazne]
Y = df["loan_status"]
# Sprawdzenie 5 pierwszych kolumn
print(df.head())

print(df[cechy_wazne].dtypes)

#Sprawdzanie jakich danych brakuje
print(df.isnull().sum())

print(df["previous_loan_defaults_on_file"].dtype)

print(df[["person_home_ownership", "previous_loan_defaults_on_file", "person_education"]].isnull().sum())

print(df.isnull().sum())
# Uzupełnianie NaN najczęściej występującą wartością
df.loc[:, "person_home_ownership"] = df["person_home_ownership"].fillna(df["person_home_ownership"].mode()[0])
df.loc[:, "person_education"] = df["person_education"].fillna(df["person_education"].mode()[0])

# Konwersja na liczby całkowite
df["person_home_ownership"] = df["person_home_ownership"].astype(int)
df["person_education"] = df["person_education"].astype(int)

# Ostateczne sprawdzenie
print(df.dtypes)  # Wszystkie powinny być int lub float
print(df.isnull().sum())  # Powinno pokazać 0 dla każdej kolumny

print(df["person_home_ownership"].unique())
print(df["person_education"].unique())

print(df.dtypes)

# Podział na zestaw treningowy i testowy (80% trening, 20% test)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=5)

# Sprawdzanie czy zbiory są zbalansowane
print(len(Y_train))
print(len(X_train))
print(len(Y_test))
print(len(X_test))

# Definiowanie siatki parametrów do przetestowania
param_grid = {
    'max_depth': [6, 8, 10, 12],
    'min_samples_split': [5, 10, 15],
    'min_samples_leaf': [2, 5, 10]
}

# GridSearchCV do znalezienia najlepszych parametrów
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=5), param_grid, cv=5)
grid_search.fit(X_train, Y_train)  # Uwaga: 'Y_train' powinno być 'y_train' zgodnie z wcześniejszym kodem

# Pobieranie najlepszego zestawu hiperparametrów
print("Najlepsze parametry:", grid_search.best_params_)

# Tworzenie i trenowanie na podstawie najlepszych parametrów
best_params = grid_search.best_params_
model = DecisionTreeClassifier(
    max_depth=best_params["max_depth"],
    min_samples_split=best_params["min_samples_split"],
    min_samples_leaf=best_params["min_samples_leaf"],
    random_state=5
)

# Ponowne trenowanie modelu
model.fit(X_train, Y_train)

# Dokładność modelu
y_pred = model.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)
print(f"Dokładność modelu po tuningu: {accuracy:.4f}")

#Sprawdzamy czy nie ma przeuczenia:
# Predykcja na zbiorze treningowym
y_train_pred = model.predict(X_train)
train_acc = accuracy_score(Y_train, y_train_pred)

# Predykcja na zbiorze testowym
y_test_pred = model.predict(X_test)
test_acc = accuracy_score(Y_test, y_test_pred)

print(f" Dokładność na zbiorze treningowym: {train_acc:.4f}")
print(f" Dokładność na zbiorze testowym: {test_acc:.4f}")

# Ważność cech
feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar', figsize=(10,5))
plt.title("Ważność cech dla modelu")
plt.show()

# Ważność cech w procentach
feature_importance = pd.Series(model.feature_importances_, index=X_train.columns)
feature_importance = feature_importance.sort_values(ascending=False) * 100  # Zamiana na procenty

# Wyświetlanie
print("Ważność cech dla modelu (%):")
print(feature_importance.to_string(float_format="%.2f"))